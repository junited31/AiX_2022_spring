---
layout: post
title: cGAN 기반 pix2pix 모델을 이용한 흑백 이미지 컬러화
---
&nbsp;

&nbsp;&nbsp; Member: 이준현, 한양대학교ERICA 응용물리학과, ljhljh0131@gmail.com

&nbsp;
## 1. Proposal
 &nbsp;

 &nbsp;&nbsp; 흑백 이미지들은 주로 오래된 기록물, 콘텐츠 등에서 다뤄진다. 대한민국 역사를 담고 있는 1950~1980년대 이미지 또한 흑백 이미지가 대다수이다. 이러한 흑백 이미지에 컬러를 입히는 Colorization 기술을 통해 자료들을 복원하는 것은 사회적/경제적 가치 측면에서 매우 의미 있는 일이며, 이에 대한 need가 나타나고 있다. 

 ![Colorization](https://user-images.githubusercontent.com/102155443/166240639-b9d43419-1e3a-496e-94af-1f252419dbd4.png)(출처: https://www.sktaifellowship.com/418f1412-55bf-46c9-8c7d-6e6c34e0ca52)

 &nbsp; SKT AI Fellowship 4기의 과제 List 중 하나로 딥러닝 기반 흑백 이미지 컬러화 기술 개발이 선정되면서, 컬러화 기술의 필요성을 실감하게 되어 대표적인 컬러화 모델인 pix2pix로 흑백 이미지 컬러화를 진행해보려 한다.

 &nbsp;

---

 &nbsp;

## 2. Datasets
 &nbsp;

 Victorian400(kaggle)

 ![Victorian400](https://user-images.githubusercontent.com/102155443/171182656-dd6843e5-81d7-49b0-9f88-d4781d326974.png)

 &nbsp;&nbsp; 이 데이터 셋은 Charles Dickens의 일러스트레이션을 채색하기 위해 만들어졌다. 그러나 데이터 부족으로 인해 이 데이터 셋의 범위는 19세기 삽화로 확장되었고, 19세기의 다양한 이미지가 흑백 이미지로 변환되어 데이터 셋에 포함되었다.

 &nbsp; 데이터 셋에는 흑백/컬러 쌍의 이미지, 그리고 이를 256*256 사이즈로 변환한 이미지가 포함되어 있다.

 &nbsp;

---

&nbsp;

## 3. Methodology
 &nbsp;

 &nbsp;&nbsp; cGAN(Conditional Generative Adversarial Networks)은 비지도학습인 GAN(Generative Adversarial Networks)의 한 종류이다. 2014년, Ian Goodfellow의 'Generative Adversarial Network'라는 논문에서 제안된 GAN은 진짜 데이터와 비슷한 데이터를 생성하는 Generator(생성기)와 generator가 생성한 데이터를 판별하는 Discriminator(판별기)로 구성되어 있다. Discriminator가 진짜 혹은 가짜 데이터인지 판별할 수 없는 경계를 GAN에서의 최적의 솔루션으로 간주한다. 이를 논문에서는 ‘경찰과 위조지폐범’을 예시로 들어 GAN 모델의 개념을 설명하고 있다. 

 ![GAN](https://user-images.githubusercontent.com/102155443/170975791-6fd07e2b-0771-4666-ac66-63ed4f4913b4.png)(출처: Hamed Alqahtani. 2019. An Analysis Of Evaluation Metric Of GANs)

 &nbsp; 일반적인 GAN의 구조에서 판별기는 실제 데이터를 진짜로 분류하도록, 생성기가 생성한 가짜 데이터를 가짜로 분류하도록 학습한다. 그리고 학습된 판별기가 가짜 데이터를 진짜로 분류하도록 생성기를 학습한다. 이는 게임이론 타입의 Value 함수로 수식화되며, 생성기와 판별기가 서로 경쟁하며 Nash equilibrium(내시 균형)에 도달하도록 하는 minmax problem이다.

 ![minmax problem_GAN](https://user-images.githubusercontent.com/102155443/170982122-a8ba8188-8c5d-46e5-9d0c-ea2c0e03a4cb.png)(출처: Ian J.Goodfellow. 2014. Generative Adversarial Nets)

 &nbsp; x~Pdata(x)는 실제 데이터에 대한 확률분포에서 샘플링한 데이터, Z~Pz(z)는 일반적으로 Zero-Mean Gaussian 분포에서의 임의의 노이즈에서 샘플링한 데이터를 의미한다. D(x)는 데이터가 진짜일 경우 1, 가짜인 경우 0의 값을 가진다. 판별기는 목적 함수 V(D, G)가 최대가 되도록 첫번째 항의 D(x), 두번째 항의 1-D(G(z))가 최대가 되어야 한다. 이는 실제 데이터를 진짜 데이터라고 분류하여 판별기 D(x)가 1이 되도록, 생성기가 생성한 가짜 데이터를 가짜로 분류하여 D(G(z))가 0이 되도록 학습하는 것을 의미한다. 반대로 생성자는 V(D, G)가 최소가 되도록 두번째 항의 D(G(z))가 1이 되도록 학습하는 것을 의미한다. 이때 첫번째 항에는 G와 독립적이므로 생략이 가능하다. 이러한 학습은 하나의 네트워크를 고정시킨 상태에서 다른 네트워크를 업데이트하는 방식으로 학습이 진행된다. 

 &nbsp; cGAN은 지도학습으로 훈련 데이터가 라벨링되어 있어야 한다는 점과, 이를 통해 인위적으로 원하는 클래스의 데이터를 샘플링할 수 있다는 점이 GAN과 다르다. 달리 말해 생성기와 판별기에 특정 condition을 나타내는 정보 y를 추가하는 것이다. 

 ![minmax problem_cGAN](https://user-images.githubusercontent.com/102155443/170992748-fddb1308-7d8b-40fd-8fb4-2ca143b54761.png)(출처: Mehdi Mirza. 2014. Conditional Generative Adversarial Nets)

 &nbsp; GAN의 목적 함수에 정보 y에 대해 조건부 확률이 추가된 점이 다르다. 

 ![structure of Pix2Pix](https://user-images.githubusercontent.com/102155443/173351435-4e08d6af-c9ed-4cd6-a498-0a0fe334d36f.PNG)(출처: Isola, Phillip. Image-to-Image Translation with Conditional Adversarial Networks)

 &nbsp; 또한, 기존의 GAN과 다르게 판별기에 생성기에서 만들어낸 것에 더해 생성기에 입력으로 주었던 것을 같이 입력하여 진짜인지 가짜인지 구별하게 된다. cGAN의 목적 함수에 위의 구조를 적용하면 아래와 같은 Pix2Pix의 adversarial loss이다. 

 ![cGAN objective func](https://user-images.githubusercontent.com/102155443/173352898-a2ab0eb9-bd52-47bf-ad8f-df034b028345.PNG)(출처: Mehdi Mirza. 2014. Conditional Generative Adversarial Nets)

 &nbsp; 판별기에 입력으로 x, y, 생성기에는 입력 x와 noise, z가 있다. 이러한 adversarial loss는 생성기가 생성하는 이미지가 진짜 이미지처럼 보이게 해준다. 이에 추가로 ground truth, y와 비슷한 이미지가 나올 수 있도록 reconstruction loss를 추가하였다. 

 ![reconstruction loss](https://user-images.githubusercontent.com/102155443/173355452-436e0c7f-4e14-4ab6-ae66-c3813bcd00f8.PNG)(출처: Isola, Phillip. Image-to-Image Translation with Conditional Adversarial Networks)

 &nbsp; 논문에 따르면 blur하지 않은, 더 나은 결과를 위해 reconstruction loss로 L1 loss를 선택하였다고 한다.

 ![Pix2Pix objective func](https://user-images.githubusercontent.com/102155443/173357515-39380a38-b780-43d1-975d-6402469a5718.PNG)(출처: Isola, Phillip. Image-to-Image Translation with Conditional Adversarial Networks)

 &nbsp; Adversarial, reconstruction loss를 추가한 최적화된 목표 함수이다. 생성기에 입력되는 noise, z를 제거하였다. 생성기의 목표 함수는 유클리드 거리를 최소화하도록 하는 L1 loss로 인해 평균, 저주파에 집중한다. 그러므로 판별기에서 고주파에 집중하도록 PatchGAN을 적용하였다. z가 제거된 이유와 PatchGAN은 3-2 학습 모델에서 후술할 것이다.

 &nbsp; cGAN 기반의 Pix2pix 모델은 Generator의 네트워크로 일반적인 Encoder-Decoder가 아닌 U-Net을 사용한다. 

 ![U-Net](https://user-images.githubusercontent.com/102155443/171179851-363787e0-18a0-4f99-ae64-de6081442936.png)(출처: Ronneberger, Olaf. 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation)

 &nbsp; FCN(Fully Convolution Network)을 토대로 확장한 U-Net은 max pooling을 통해 feature map을 축소시켜 downsampling하는 encoder, 축소시킨 feature map을 up convolution을 통해 확장시켜 upsampling하는 decoder, 그리고 이 둘을 Concatenate를 통해 연결시키는 skip connection으로 구성되어 있다. Skip connection을 통해 네트워크 얕은 층에서 얻을 수 있는 fine, 국소적인 정보, 깊은 층에서 얻을 수 있는 global, 전역적인 정보를 모두 포함할 수 있다.
 &nbsp; 

 &nbsp;
## 3-1 Data Preprocessing
 &nbsp;

 &nbsp;&nbsp; Victorian400 데이터셋에는 각각 400개의 이미지가 저장되어 있다. 이론적으로 이미지의 픽셀값을 정규화하면 신경망 모델이 빠르게 수렴한다고 한다. 따라서 각 이미지의 채널별 평균값과 표준편차 값으로 정규화하여 데이터 전처리를 진행하였다.

 ```python
 color_mean = [0.58090717, 0.52688643, 0.45678478]
 color_std = [0.25644188, 0.25482641, 0.24456465]
 gray_mean = [0.5350533, 0.5350533, 0.5350533]
 gray_std = [0.25051587, 0.25051587, 0.25051587]

 color_transforms_ = [
    transforms.ToTensor(),
    transforms.Normalize(mean=color_mean, std=color_std),
 ]

 gray_transforms_ = [
    transforms.ToTensor(),
    transforms.Normalize(mean=gray_mean, std=gray_std),
 ]
 ```

 &nbsp; 이때 평균과 표준편차의 값은 각각의 픽셀값을 255로 나누어 0~1 사이의 값으로 변환시킨 후 구하였다. 이는 transforms.ToTensor() 함수로 이미지를 텐서 형태로 변환시킬 때, 픽셀값이 0~1 사이의 값을 반환하기 때문이다.



 &nbsp;
## 3-2 학습 모델
 &nbsp;

 &nbsp;&nbsp; 

 ```python
 def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

 # U-NET

 class UNetDown(nn.Module):
    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):
        super(UNetDown, self).__init__()
        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]
        if normalize:
            layers.append(nn.InstanceNorm2d(out_size))
        layers.append(nn.LeakyReLU(0.2))
        if dropout:
            layers.append(nn.Dropout(dropout))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


 class UNetUp(nn.Module):
    def __init__(self, in_size, out_size, dropout=0.0):
        super(UNetUp, self).__init__()
        layers = [
            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(out_size),
            nn.ReLU(inplace=True),
        ]
        if dropout:
            layers.append(nn.Dropout(dropout))

        self.model = nn.Sequential(*layers)

    def forward(self, x, skip_input):
        x = self.model(x)
        x = torch.cat((x, skip_input), 1)
        return x
 ```

 &nbsp; Downsampling layer는 convolution, normalization, activation function, dropout으로 구성되어 있다. Convolution을 통해 채널의 수가 out_size만큼 늘어나고, feature map은 width, height 둘 다 1/2만큼 줄어든다. 

 &nbsp; Upsampling layer는 transpose convolution, normalization, activation function, dropout으로 구성되어 있다. Downsampling layer와는 반대로 Transpose convolution을 통해 채널의 수는 out_size만큼 줄어들고, feature map은 width, height 둘 다 2배 늘어난다. Torch.cat을 통해 skip connection된 input의 채널을 합쳐준다. 이는 torch의 tensor가 channel first의 형태로 (batch_size, channel, width, height)이므로, dim=1은 채널을 의미하기 때문이다.

 ```python
 class GeneratorUNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3):
        super(GeneratorUNet, self).__init__()
        
        self.down1 = UNetDown(in_channels, 64, normalize=False)
        self.down2 = UNetDown(64, 128)
        self.down3 = UNetDown(128, 256)
        self.down4 = UNetDown(256, 512, dropout=0.5)
        self.down5 = UNetDown(512, 512, dropout=0.5)
        self.down6 = UNetDown(512, 512, dropout=0.5)
        self.down7 = UNetDown(512, 512, dropout=0.5)
        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)

        self.up1 = UNetUp(512, 512, dropout=0.5)
        self.up2 = UNetUp(1024, 512, dropout=0.5)
        self.up3 = UNetUp(1024, 512, dropout=0.5)
        self.up4 = UNetUp(1024, 512, dropout=0.5)
        self.up5 = UNetUp(1024, 256)
        self.up6 = UNetUp(512, 128)
        self.up7 = UNetUp(256, 64)

        self.final = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(128, out_channels, 4, padding=1),
            nn.Tanh(),
        )

    def forward(self, x):
        # Skip connections from encoder to decoder
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        d6 = self.down6(d5)
        d7 = self.down7(d6)
        d8 = self.down8(d7)
        u1 = self.up1(d8, d7)
        u2 = self.up2(u1, d6)
        u3 = self.up3(u2, d5)
        u4 = self.up4(u3, d4)
        u5 = self.up5(u4, d3)
        u6 = self.up6(u5, d2)
        u7 = self.up7(u6, d1)

        return self.final(u7)
 ```

 &nbsp; U-Net을 사용하는 generator의 down, upsampling layer인 encoder, decoder는 위처럼 skip connection을 통해 연결된다. 일반적으로 GAN의 generator는 random한 noise, z를 추가로 입력하여 deterministic하지 않은 출력을 생성하는데, pix2pix에서는 z를 추가하는 것을 볼 수 없다. 이는 GAN은 학습된 generator에 z를 입력하여 다양한 화면을 만드는 것이 목적이지만, Pix2Pix 모델은 이전 화면을 통해 다음 화면을 예측하는 것이 목적이기 때문이다. 이전 화면은 다음 화면과의 연관관계를 가지며, 일종의 condition 역할을 하기 때문에 z의 의미가 무색해졌다고 할 수 있으며, 논문에 따르면 z의 유무는 미미한 영향을 끼칠 뿐이라고 한다. z를 제거하면서 나타날 수 있는 deterministic한 출력은 down, upsampling layer의 dropout의 확률적 무작위성을 이용하여 피하였다. 논문에서는 gaussian noise z를 입력하여 진행해보기도 했는데, 효과적이지 않으며 판별기가 noise를 무시하는 것을 확인하였다고 한다. 그러나 z를 제거하고 dropout 층을 추가하며 stochasticity가 떨어지는 것을 확인하였고 이를 해결하는 것은 future work에 진행한다고 한다.

 ```python
 class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super(Discriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, normalization=True):
            """Returns downsampling layers of each discriminator block"""
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalization:
                layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *discriminator_block(in_channels * 2, 64, normalization=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1, bias=False)
        )

    def forward(self, img_A, img_B):
        # Concatenate image and condition image by channels to produce input
        img_input = torch.cat((img_A, img_B), 1)
        return self.model(img_input)
 ```

 &nbsp; 판별기는 channel이 같은 생성기가 생성한 이미지, ground truth인 이미지가 입력된다. 판별기의 discriminator_block은 총 4개로 구성되어 있는데 256 * 256 크기의 입력이 16 * 16의 출력이 되는 것을 알 수 있으며, 16 * 16의 출력을 patch라 한다.

 ```python
 n_epochs = 100
 dataset_name = "Victorian400"
 lr = 0.0002
 b1 = 0.5                    # adam: decay of first order momentum of gradient
 b2 = 0.999                  # adam: decay of first order momentum of gradient
 decay_epoch = 100           # epoch from which to start lr decay
 n_cpu = 6                   # number of cpu threads to use during batch generation
 channels = 3                # number of image channels
 checkpoint_interval = 20    # interval between model checkpoints
 ```
 &nbsp; 학습에 사용할 파라미터들을 설정하였다.

 ```python
 # Loss functions
 criterion_GAN = torch.nn.MSELoss()
 criterion_pixelwise = torch.nn.L1Loss()

 # Loss weight of L1 pixel-wise loss between translated image and real image
 lambda_pixel = 100

 # Calculate output of image discriminator (PatchGAN)
 patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)

 # Initialize generator and discriminator
 generator = GeneratorUNet()
 discriminator = Discriminator()

 cuda = True if torch.cuda.is_available() else False

 if cuda:
    generator = generator.cuda()
    discriminator = discriminator.cuda()
    criterion_GAN.cuda()
    criterion_pixelwise.cuda()

 # Optimizers
 optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))
 optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))

 # Tensor type
 Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor
 ```
 &nbsp; 변수 patch는 (1, 16, 16)인 것을 확인할 수 있다.
 ```python
 # ----------
 #  Training
 # ----------

 for epoch in range(1, n_epochs+1):
    start_time = datetime.datetime.now()
    for i, batch in enumerate(train_loader):

        # Model inputs
        gray = Variable(batch["A"].type(Tensor))
        color = Variable(batch["B"].type(Tensor))

        # Adversarial ground truths
        valid = Variable(Tensor(np.ones((gray.size(0), *patch))), requires_grad=False)
        fake = Variable(Tensor(np.zeros((gray.size(0), *patch))), requires_grad=False)

        # ------------------
        #  Train Generators
        # ------------------

        optimizer_G.zero_grad()

        # GAN loss
        output = generator(gray)
        pred_fake = discriminator(output, gray)
        loss_GAN = criterion_GAN(pred_fake, valid)
        # Pixel-wise loss
        loss_pixel = criterion_pixelwise(output, color)

        # Total loss
        loss_G = loss_GAN + lambda_pixel * loss_pixel

        loss_G.backward()

        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_D.zero_grad()

        # Real loss
        pred_real = discriminator(color, gray)
        loss_real = criterion_GAN(pred_real, valid)

        # Fake loss
        pred_fake = discriminator(output.detach(), gray)
        loss_fake = criterion_GAN(pred_fake, fake)

        # Total loss
        loss_D = 0.5 * (loss_real + loss_fake)

        loss_D.backward()
        optimizer_D.step()

        epoch_time = datetime.datetime.now() - start_time

    if (epoch) % checkpoint_interval == 0:
        fig = plt.figure(figsize=(18, 18))
        sample_images(epoch, train_loader, 'val')

        torch.save(generator.state_dict(), "saved_models/%s/generator_%d.pth" % (dataset_name, epoch))
        torch.save(discriminator.state_dict(), "saved_models/%s/discriminator_%d.pth" % (dataset_name, epoch))

        print("[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s" % (epoch, 
                                                                                                    n_epochs, 
                                                                                                    i+1, 
                                                                                                    len(train_loader), 
                                                                                                    loss_D.item(), 
                                                                                                    loss_G.item(), 
                                                                                                    loss_pixel.item(), 
                                                                                                    loss_GAN.item(), 
                                                                                                    epoch_time))     
 ```
 
 &nbsp; Ground truth인 gray 이미지를 patch 사이즈인 16 * 16 크기로 바꾸었고, 이는 판별기에서의 출력과 동일하다. 이를 통해 각 patch를 비교하여 loss 값을 구하게 된다. 이러한 PatchGAN discriminator는 patch 사이즈 이상으로 떨어진 픽셀 사이의 독립성을 가정하여, Markov random field로 모델링한다. Patch가 많을수록, 즉 사이즈가 작을수록 파라미터가 적어지고 다양한 크기의 이미지에도 적용할 수 있다. 

---

&nbsp;
## 4. Evaluation & Analysis
&nbsp;

 ```python
 generator.eval()
 discriminator.eval()

 fig = plt.figure(figsize=(18,10))
 sample_images(n_epochs, test_loader, 'test')
 ```
 ![result1](https://user-images.githubusercontent.com/102155443/171175452-6e73e1ce-b8b0-4da5-a06f-1e2307b5dba8.png)

 ![result2-1](https://user-images.githubusercontent.com/102155443/171175937-3b975753-9cab-41cc-8d4f-1c4bfe02f2b6.png)


 &nbsp;&nbsp;위에서부터 채색전의 흑백이미지, 목표로하는 컬러이미지, epoch = 20, 100에서의 학습된 모델로 채색한 이미지이다. epoch = 100에서의 학습된 모델로 채색한 이미지가 더욱 다채로운 색으로 채색된 것을 볼 수 있다.

 ![result2-2](https://user-images.githubusercontent.com/102155443/173372829-f09cee9d-129a-4e4e-8a0d-559c431a4f8d.png)

 &nbsp; Pix2Pix 모델이 아닌 일반적인 GAN 모델을 이용하여 Victorian400을 채색한 것이다. Pix2Pix 모델의 성능이 더 나은 것을 확인할 수 있다. 

 &nbsp;

---

&nbsp;
## 5. Related Works
&nbsp;

 &nbsp; Isola, P., Zhu, J.-Y., Zhou, T., &amp; Efros, A. A. (2018, November 26). Image-to-image translation with conditional adversarial networks. arXiv.org. Retrieved May 31, 2022, from https://arxiv.org/abs/1611.07004 

 &nbsp; Ronneberger, Olaf, et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv.org, 18 May 2015, https://arxiv.org/abs/1505.04597. 

 &nbsp; Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014, June 10). Generative Adversarial Networks. arXiv.org. Retrieved June 13, 2022, from https://arxiv.org/abs/1406.2661 

&nbsp;
## 5-1. Related blogs
&nbsp;

 &nbsp; https://pseudo-lab.github.io/Tutorial-Book/chapters/GAN/Ch4-pix2pix.html

 &nbsp; https://blog.naver.com/PostView.naver?blogId=laonple&logNo=221356582945&parentCategoryNo=&categoryNo=22&viewDate=&isShowPopularPosts=false&from=postView

 &nbsp;

---

&nbsp;
## 6. Discussion & Result
&nbsp;

 &nbsp;&nbsp; Patch size를 조절하여 모델을 학습하는 것은 모델의 성능을 높이기 좋아보인다. 하지만 patch size가 작아지게 된다면 Markov random field로 모델링할 때의 가정인 patch 사이즈 이상으로 떨어진 픽셀 사이의 독립성에 대해 생각해보아야 할 것이다.
 
 &nbsp; cGAN 기반의 Pix2Pix 모델을 이용하여 Victorian400을 채색하였다. U-net을 이용하여 네트워크 깊이에 따른 trade-off를 완화시켰으며, patchGAN으로 전체 크기의 이미지가 아닌 patch 사이즈를 판별기로 분류함으로써 모델의 성능을 높였다. 이를 통해 epoch가 진행됨에 따라 채색의 완성도가 높아지는 것을 확인하였다. 

&nbsp;

---

&nbsp;
## 7. Future work
&nbsp;

 ![result3](https://user-images.githubusercontent.com/102155443/173374571-868c8f6a-64e7-4434-bf64-5ec0407f15ec.png)

 &nbsp;&nbsp; patch size를 16 * 16에서 8 * 8로 바꾸어 학습한 모델이다. 점점 특정한 색(빨강, 노랑)에서 잘못 학습된 것을 확인할 수 있었다. 이는 더 이상 Markov random field로 모델링할 수 없다는 것을 의미한다고 생각한다. 

 &nbsp; Patch size이외에도 batch size, kernal size 등 parameter를 조절하여 모델의 성능을 높이는 것도 좋은 방법이 될 수 있으며, 이는 다음에 진행할 예정이다.