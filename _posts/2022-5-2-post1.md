---
layout: post
title: cGAN 기반 pix2pix 모델을 이용한 흑백 이미지 컬러화
---
&nbsp;

&nbsp;&nbsp; Member: 이준현, 한양대학교ERICA 응용물리학과, ljhljh0131@gmail.com

&nbsp;
## 1. Proposal
 &nbsp;

 &nbsp;&nbsp; 흑백 이미지들은 주로 오래된 기록물, 콘텐츠 등에서 다뤄진다. 대한민국 역사를 담고 있는 1950~1980년대 이미지 또한 흑백 이미지가 대다수이다. 이러한 흑백 이미지에 컬러를 입히는 Colorization 기술을 통해 자료들을 복원하는 것은 사회적/경제적 가치 측면에서 매우 의미 있는 일이며, 이에 대한 need가 나타나고 있다. 

 ![Colorization](https://user-images.githubusercontent.com/102155443/166240639-b9d43419-1e3a-496e-94af-1f252419dbd4.png)(출처: https://www.sktaifellowship.com/418f1412-55bf-46c9-8c7d-6e6c34e0ca52)

 &nbsp; SKT AI Fellowship 4기의 과제 List 중 하나로 딥러닝 기반 흑백 이미지 컬러화 기술 개발이 선정되면서, 컬러화 기술의 필요성을 실감하게 되어 대표적인 컬러화 모델인 pix2pix로 흑백 이미지 컬러화를 진행해보려 한다.

 &nbsp;

---

 &nbsp;

## 2. Datasets
 &nbsp;

 Victorian400(kaggle)

 ![Victorian400](https://user-images.githubusercontent.com/102155443/171182656-dd6843e5-81d7-49b0-9f88-d4781d326974.png)

 &nbsp;&nbsp; 이 데이터 셋은 Charles Dickens의 일러스트레이션을 채색하기 위해 만들어졌다. 그러나 데이터 부족으로 인해 이 데이터 셋의 범위는 19세기 삽화로 확장되었고, 19세기의 다양한 이미지가 흑백 이미지로 변환되어 데이터 셋에 포함되었다.

 &nbsp; 데이터 셋에는 흑백/컬러 쌍의 이미지, 그리고 이를 256*256 사이즈로 변환한 이미지가 포함되어 있다.

 &nbsp;

---

&nbsp;

## 3. Methodology
 &nbsp;

 &nbsp;&nbsp; cGAN(Conditional Generative Adversarial Networks)은 비지도학습인 GAN(Generative Adversarial Networks)의 한 종류이다. 2014년, Ian Goodfellow의 'Generative Adversarial Network'라는 논문에서 제안된 GAN은 진짜 데이터와 비슷한 데이터를 생성하는 Generator(생성기)와 generator가 생성한 데이터를 판별하는 Discriminator(판별기)로 구성되어 있다. Discriminator가 진짜 혹은 가짜 데이터인지 판별할 수 없는 경계를 GAN에서의 최적의 솔루션으로 간주한다. 이를 논문에서는 ‘경찰과 위조지폐범’을 예시로 들어 GAN 모델의 개념을 설명하고 있다. 

 ![GAN](https://user-images.githubusercontent.com/102155443/170975791-6fd07e2b-0771-4666-ac66-63ed4f4913b4.png)(출처: Hamed Alqahtani. 2019. An Analysis Of Evaluation Metric Of GANs)

 &nbsp; 일반적인 GAN의 구조에서 판별기는 실제 데이터를 진짜로 분류하도록, 생성기가 생성한 가짜 데이터를 가짜로 분류하도록 학습한다. 그리고 학습된 판별기가 가짜 데이터를 진짜로 분류하도록 생성기를 학습한다. 이는 게임이론 타입의 Value 함수로 수식화되며, 생성기와 판별기가 서로 경쟁하며 Nash equilibrium(내시 균형)에 도달하도록 하는 minmax problem이다.

 ![minmax problem_GAN](https://user-images.githubusercontent.com/102155443/170982122-a8ba8188-8c5d-46e5-9d0c-ea2c0e03a4cb.png)(출처: Ian J.Goodfellow. 2014. Generative Adversarial Nets)

 &nbsp; x~Pdata(x)는 실제 데이터에 대한 확률분포에서 샘플링한 데이터, Z~Pz(z)는 일반적으로 Zero-Mean Gaussian 분포에서의 임의의 노이즈에서 샘플링한 데이터를 의미한다. D(x)는 데이터가 진짜일 경우 1, 가짜인 경우 0의 값을 가진다. 판별기는 value 함수 V(D, G)가 최대가 되도록 첫번째 항의 D(x), 두번째 항의 1-D(G(z))가 최대가 되어야 한다. 이는 실제 데이터를 진짜 데이터라고 분류하여 판별기 D(x)가 1이 되도록, 생성기가 생성한 가짜 데이터를 가짜로 분류하여 D(G(z))가 0이 되도록 학습하는 것을 의미한다. 반대로 생성자는 V(D, G)가 최소가 되도록 두번째 항의 D(G(z))가 1이 되도록 학습하는 것을 의미한다. 이때 첫번째 항에는 G와 독립적이므로 생략이 가능하다. 이러한 학습은 하나의 네트워크를 고정시킨 상태에서 다른 네트워크를 업데이트하는 방식으로 학습이 진행된다. 

 &nbsp; cGAN은 지도학습으로 훈련 데이터가 라벨링되어 있어야 한다는 점과, 이를 통해 인위적으로 원하는 클래스의 데이터를 샘플링할 수 있다는 점이 GAN과 다르다. 달리 말해 생성기와 판별기에 특정 condition을 나타내는 정보 y를 추가하는 것이다. 

 ![minmax problem_cGAN](https://user-images.githubusercontent.com/102155443/170992748-fddb1308-7d8b-40fd-8fb4-2ca143b54761.png)(출처: Mehdi Mirza. 2014. Conditional Generative Adversarial Nets)

 &nbsp; GAN의 value 함수에 정보 y에 대해 조건부 확률이 추가된 점이 다르다.

 &nbsp; cGAN 기반의 Pix2pix 모델은 Generator의 네트워크로 일반적인 Encoder-Decoder가 아닌 U-Net을 사용한다. 

 ![U-Net](https://user-images.githubusercontent.com/102155443/171179851-363787e0-18a0-4f99-ae64-de6081442936.png)(출처: Ronneberger, Olaf. 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation)

 &nbsp; 


 &nbsp;U-net, up/down sampling, pix2pix

 &nbsp;
## 3-1 Data Preprocessing
 &nbsp;

 &nbsp;&nbsp; Victorian400 데이터셋에는 각각 400개의 이미지가 저장되어 있다. 이론적으로 이미지의 픽셀값을 정규화하면 신경망 모델이 빠르게 수렴한다고 한다. 따라서 각 이미지의 채널별 평균값과 표준편차 값으로 정규화하여 데이터 전처리를 진행하였다.

 ```python
 color_mean = [0.58090717, 0.52688643, 0.45678478]
 color_std = [0.25644188, 0.25482641, 0.24456465]
 gray_mean = [0.5350533, 0.5350533, 0.5350533]
 gray_std = [0.25051587, 0.25051587, 0.25051587]

 color_transforms_ = [
    transforms.ToTensor(),
    transforms.Normalize(mean=color_mean, std=color_std),
 ]

 gray_transforms_ = [
    transforms.ToTensor(),
    transforms.Normalize(mean=gray_mean, std=gray_std),
 ]
 ```

 &nbsp; 이때 평균과 표준편차의 값은 픽셀값을 255로 나누어 0~1 사이의 값으로 변환시킨 후 구하였다. 이는 transforms.ToTensor() 함수로 이미지를 텐서 형태로 변환시킬 때, 픽셀값이 0~1 사이의 값을 반환하기 때문이다.



 &nbsp;
## 3-2 학습 모델
 &nbsp;

 &nbsp;&nbsp; pix2pix 모델의 특징은 일반적인 Encoder-Decoder가 아닌 U-Net을 사용한다는 것이다. 

 ```python
 def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

 # U-NET 생성

 class UNetDown(nn.Module):
    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):
        super(UNetDown, self).__init__()
        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]
        if normalize:
            layers.append(nn.InstanceNorm2d(out_size))
        layers.append(nn.LeakyReLU(0.2))
        if dropout:
            layers.append(nn.Dropout(dropout))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


 class UNetUp(nn.Module):
    def __init__(self, in_size, out_size, dropout=0.0):
        super(UNetUp, self).__init__()
        layers = [
            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(out_size),
            nn.ReLU(inplace=True),
        ]
        if dropout:
            layers.append(nn.Dropout(dropout))

        self.model = nn.Sequential(*layers)

    def forward(self, x, skip_input):
        x = self.model(x)
        x = torch.cat((x, skip_input), 1)
        return x


 class GeneratorUNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3):
        super(GeneratorUNet, self).__init__()
        
        self.down1 = UNetDown(in_channels, 64, normalize=False)
        self.down2 = UNetDown(64, 128)
        self.down3 = UNetDown(128, 256)
        self.down4 = UNetDown(256, 512, dropout=0.5)
        self.down5 = UNetDown(512, 512, dropout=0.5)
        self.down6 = UNetDown(512, 512, dropout=0.5)
        self.down7 = UNetDown(512, 512, dropout=0.5)
        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)

        self.up1 = UNetUp(512, 512, dropout=0.5)
        self.up2 = UNetUp(1024, 512, dropout=0.5)
        self.up3 = UNetUp(1024, 512, dropout=0.5)
        self.up4 = UNetUp(1024, 512, dropout=0.5)
        self.up5 = UNetUp(1024, 256)
        self.up6 = UNetUp(512, 128)
        self.up7 = UNetUp(256, 64)

        self.final = nn.Sequential(
            nn.Upsample(scale_factor=2),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(128, out_channels, 4, padding=1),
            nn.Tanh(),
        )

    def forward(self, x):
        # U-Net generator with skip connections from encoder to decoder
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        d6 = self.down6(d5)
        d7 = self.down7(d6)
        d8 = self.down8(d7)
        u1 = self.up1(d8, d7)
        u2 = self.up2(u1, d6)
        u3 = self.up3(u2, d5)
        u4 = self.up4(u3, d4)
        u5 = self.up5(u4, d3)
        u6 = self.up6(u5, d2)
        u7 = self.up7(u6, d1)

        return self.final(u7)
 ```

 ```python
 class Discriminator(nn.Module):
    def __init__(self, in_channels=3):
        super(Discriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, normalization=True):
            """Returns downsampling layers of each discriminator block"""
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalization:
                layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *discriminator_block(in_channels * 2, 64, normalization=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1, bias=False)
        )

    def forward(self, img_A, img_B):
        # Concatenate image and condition image by channels to produce input
        img_input = torch.cat((img_A, img_B), 1)
        return self.model(img_input)
 ```

 ```python
 n_epochs = 100
 dataset_name = "Victorian400"
 lr = 0.0002
 b1 = 0.5                    # adam: decay of first order momentum of gradient
 b2 = 0.999                  # adam: decay of first order momentum of gradient
 decay_epoch = 100           # epoch from which to start lr decay
 n_cpu = 6                   # number of cpu threads to use during batch generation
 channels = 3                # number of image channels
 checkpoint_interval = 20    # interval between model checkpoints
 ```
 &nbsp; 학습에 사용할 파라미터들을 설정하였다.

 ```python
 # Loss functions
 criterion_GAN = torch.nn.MSELoss()
 criterion_pixelwise = torch.nn.L1Loss()

 # Loss weight of L1 pixel-wise loss between translated image and real image
 lambda_pixel = 100

 # Calculate output of image discriminator (PatchGAN)
 patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)

 # Initialize generator and discriminator
 generator = GeneratorUNet()
 discriminator = Discriminator()

 cuda = True if torch.cuda.is_available() else False

 if cuda:
    generator = generator.cuda()
    discriminator = discriminator.cuda()
    criterion_GAN.cuda()
    criterion_pixelwise.cuda()

 # Optimizers
 optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))
 optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))

 # Tensor type
 Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor
 ```
 ```python
 # ----------
 #  Training
 # ----------

 for epoch in range(1, n_epochs+1):
    start_time = datetime.datetime.now()
    for i, batch in enumerate(train_loader):

        # Model inputs
        gray = Variable(batch["A"].type(Tensor))
        color = Variable(batch["B"].type(Tensor))

        # Adversarial ground truths
        valid = Variable(Tensor(np.ones((gray.size(0), *patch))), requires_grad=False)
        fake = Variable(Tensor(np.zeros((gray.size(0), *patch))), requires_grad=False)

        # ------------------
        #  Train Generators
        # ------------------

        optimizer_G.zero_grad()

        # GAN loss
        output = generator(gray)
        pred_fake = discriminator(output, gray)
        loss_GAN = criterion_GAN(pred_fake, valid)
        # Pixel-wise loss
        loss_pixel = criterion_pixelwise(output, color)

        # Total loss
        loss_G = loss_GAN + lambda_pixel * loss_pixel

        loss_G.backward()

        optimizer_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_D.zero_grad()

        # Real loss
        pred_real = discriminator(color, gray)
        loss_real = criterion_GAN(pred_real, valid)

        # Fake loss
        pred_fake = discriminator(output.detach(), gray)
        loss_fake = criterion_GAN(pred_fake, fake)

        # Total loss
        loss_D = 0.5 * (loss_real + loss_fake)

        loss_D.backward()
        optimizer_D.step()

        epoch_time = datetime.datetime.now() - start_time

    if (epoch) % checkpoint_interval == 0:
        fig = plt.figure(figsize=(18, 18))
        sample_images(epoch, train_loader, 'val')

        torch.save(generator.state_dict(), "saved_models/%s/generator_%d.pth" % (dataset_name, epoch))
        torch.save(discriminator.state_dict(), "saved_models/%s/discriminator_%d.pth" % (dataset_name, epoch))

        print("[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s" % (epoch, 
                                                                                                    n_epochs, 
                                                                                                    i+1, 
                                                                                                    len(train_loader), 
                                                                                                    loss_D.item(), 
                                                                                                    loss_G.item(), 
                                                                                                    loss_pixel.item(), 
                                                                                                    loss_GAN.item(), 
                                                                                                    epoch_time))     
 ```
 
 &nbsp;

---

&nbsp;
## 4. Evaluation & Analysis
&nbsp;

 ```python
 generator.eval()
 discriminator.eval()

 fig = plt.figure(figsize=(18,10))
 sample_images(n_epochs, test_loader, 'test')
 ```
 ![result1](https://user-images.githubusercontent.com/102155443/171175452-6e73e1ce-b8b0-4da5-a06f-1e2307b5dba8.png)

 ![result2-1](https://user-images.githubusercontent.com/102155443/171175937-3b975753-9cab-41cc-8d4f-1c4bfe02f2b6.png)


 &nbsp;&nbsp;위에서부터 채색전의 흑백이미지, 목표로하는 컬러이미지, epoch = 20, 100에서의 학습된 모델로 채색한 이미지이다. epoch = 100에서의 학습된 모델로 채색한 이미지가 더욱 다채로운 색으로 채색된 것을 볼 수 있다.   

 &nbsp;

---

&nbsp;
## 5. Related Works
&nbsp;

 &nbsp; Isola, P., Zhu, J.-Y., Zhou, T., &amp; Efros, A. A. (2018, November 26). Image-to-image translation with conditional adversarial networks. arXiv.org. Retrieved May 31, 2022, from https://arxiv.org/abs/1611.07004 

 &nbsp; Ronneberger, Olaf, et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” ArXiv.org, 18 May 2015, https://arxiv.org/abs/1505.04597. 

 &nbsp; 

---

&nbsp;
## 6. Discussion
&nbsp;